%% LyX 2.3.5.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,hebrew,english]{article}
\usepackage[T1]{fontenc}
\usepackage[cp1255,latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{float}
\usepackage{calc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\PassOptionsToPackage{normalem}{ulem}
\usepackage{ulem}
\setstretch{1.5}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% Hebrew does not work unless I use this. 
\usepackage{culmus}

\usepackage{fancyhdr}
\fancyhf{}               % Clear fancy header/footer
\fancyhead[C]{
\begin{otherlanguage}{English}
Technion, Faculty of Industrial Engineering and Management
\end{otherlanguage}
}   
\fancyfoot[C]{\thepage}  % Page number in Right footer
\makeatletter
\let\ps@plain\ps@fancy   % Plain page style = fancy page style
\makeatother

\makeatother

\usepackage{babel}
\begin{document}
\title{Optimization 1 - 098311\inputencoding{cp1255}\R{}\\
\inputencoding{latin9}Winter 2021 - HW 11}
\author{Ido Czerninski 312544596, Asaf Gendler 301727715\inputencoding{cp1255}\R{ }}

\maketitle
\selectlanguage{hebrew}%
\inputencoding{cp1255}\newpage{}
\selectlanguage{english}%

\section*{\uline{Problem 1:}}

\inputencoding{latin9}For $a\in\mathbb{R}$ consider the problem:
\begin{align*}
\min_{x,y,z\in\mathbb{R}} & f\left(x,y,z\right)=x-4y+az^{4}\\
s.t & x+y+z^{2}\leq2\\
 & x\geq0\\
 & y\geq0
\end{align*}


\subsection*{a)}

define: 
\[
X=\left\{ \left(x,y,z\right)\in\mathbb{R}:x\geq0,y\geq0\right\} 
\]

first we will write the Lagrangian and give Lagrange multiplier only
to the first constraint:
\[
L\left(x,y,z,\lambda\right)=x-4y+az^{4}+\lambda\left(x+y+z^{2}-2\right),\lambda\geq0
\]
now let's find the dual problem:
\begin{align*}
q\left(\lambda\right) & =\min_{\left(x,y,z\right)\in X}L\left(x,y,z,\lambda\right)=\min_{x\geq0,y\geq0,z}x-4y+az^{4}+\lambda\left(x+y+z^{2}-2\right)=\\
 & =\min_{x\geq0,y\geq0,z}x+\lambda x-4y+\lambda y+az^{4}+\lambda z^{2}-2\lambda=\\
 & =\min_{x\geq0}x+\lambda x+\min_{y\geq0}-4y+\lambda y+\min_{z\in\mathbb{R}}az^{4}+\lambda z^{2}-2\lambda=\\
 & =\min_{x\geq0}\left(\lambda+1\right)x+\min_{y\geq0}\left(\lambda-4\right)y+\min_{z\in\mathbb{R}}az^{4}+\lambda z^{2}-2\lambda
\end{align*}
\[
\min_{x\geq0}\left(\lambda+1\right)x=0
\]
\[
\min_{y\geq0}\left(\lambda-4\right)y=\begin{cases}
0 & \lambda\geq4\\
-\infty & \lambda<4
\end{cases}
\]
\[
\min_{z\in\mathbb{R}}az^{4}+\lambda z^{2}=\begin{cases}
0 & a\geq0\\
-\infty & a<0
\end{cases}
\]

\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
thus if $a<0$ $dom\left(q\right)=\emptyset$ and the dual problem
is not feasible.

for $a\geq0$ the dual problem is:

\begin{align*}
\max_{\lambda} & -2\lambda\\
s.t & \lambda\geq4
\end{align*}
%
\end{minipage}}

first notice that the function is continuously differentiable, and
the constraints define a compact set, thus according to Weierstrass
therefrom, a minimum value is attained.

$f\left(x\right)=x^{4}$ is a convex function for all $x$ hence $\left(\left(\begin{array}{ccc}
0 & 0 & 1\end{array}\right)\left(\begin{array}{c}
x\\
y\\
z
\end{array}\right)\right)^{4}=z^{4}$ is a convex function as a linear change in the coordinates.

thus for $a\geq0$ $az^{4}$ is a convex function and concave for
$a<0$

$x-4y$ is convex as a linear function.

thus $f\left(x,y,z\right)$ is convex for $a\geq0$ and concave for
$a<0$

the first constraint is convex as a level set of a quadratic function
with a P.D matrix.

all the other constraints are convex as linear constraints.

thus for $a\geq0$ the problem is a convex optimization problem.

Slater condition holds, for example for $x=y=0,z=1$
\[
x+y+z^{2}=1<2
\]

\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
thus for $a\geq0$ the problem is convex, feasible and salter condition
holds, thus strong duality is guaranteed.%
\end{minipage}}

for $a<0$ the problem is not convex and of course duality is not
guaranteed.

\subsection*{b)}

for $a=-1$ we saw that the dual problem is not feasible and thus
doesn't have a solution.

\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
for $a=1$ the dual problem is:
\begin{align*}
\max_{\lambda} & -2\lambda\\
s.t & \lambda\geq4
\end{align*}
the optimal value is achieved at $\lambda^{*}=4$ and the value is
$-8$.%
\end{minipage}}

since strong duality holds we can find the optimal solution of the
primal problem as well:
\begin{align*}
\left(x^{*},y^{*},z^{*}\right) & \in\arg\min_{\left(x,y,z\right)\in X}L\left(x,y,z,\lambda^{*}\right)
\end{align*}
\[
\lambda^{*}\left(x+y+z^{2}-2\right)=0
\]
\[
L\left(x,y,z,\lambda^{*}\right)=5x+az^{4}+4z^{2}-8
\]
\[
x^{*}=z^{*}=0
\]
\[
x+y+z^{2}-2=y-2=0
\]
\[
y^{*}=2
\]
and indeed the function value is:
\[
f\left(0,2,0\right)=-8=q\left(4\right)
\]

\selectlanguage{hebrew}%
\inputencoding{cp1255}\newpage{}
\selectlanguage{english}%

\section*{\uline{Problem 2:}}

\inputencoding{latin9}Find a dual to the problem:
\begin{align*}
\min_{x\in\mathbb{R}^{n}} & \sum_{i=1}^{n}\left(x_{i}\ln\left(x_{i}\right)-x_{i}\right)\\
s.t & Ax\leq b\\
 & x>0
\end{align*}

where $A\in\mathbb{R}^{mxn}$,$b\in\mathbb{R}^{m}$

define:
\[
X=\left\{ x\in\mathbb{R}^{n}:x>0\right\} 
\]
define the Lagrangian:
\[
L\left(x,\lambda\right)=\sum_{i=1}^{n}\left(x_{i}\ln\left(x_{i}\right)-x_{i}\right)+\lambda^{T}\left(Ax-b\right)
\]
\[
\lambda\in\mathbb{R}^{m},\lambda\geq0
\]
let's find the dual problem:
\begin{align*}
q\left(\lambda\right) & =\min_{x\in X}L\left(x,\lambda\right)=\min_{x\in X}\sum_{i=1}^{n}\left(x_{i}\ln\left(x_{i}\right)-x_{i}\right)+\lambda^{T}\left(Ax-b\right)=\\
 & =\min_{x\in X}\sum_{i=1}^{n}\left(x_{i}\ln\left(x_{i}\right)-x_{i}\right)+\lambda^{T}Ax-\lambda^{T}b=\\
 & =\min_{x\in X}\sum_{i=1}^{n}\left(x_{i}\ln\left(x_{i}\right)-x_{i}\right)+\sum_{i=1}^{n}\lambda^{T}A_{:,i}x_{i}-\lambda^{T}b=\\
 & =\min_{x\in X}\sum_{i=1}^{n}\left(x_{i}\ln\left(x_{i}\right)-x_{i}+\lambda^{T}A_{:,i}x_{i}\right)-\lambda^{T}b
\end{align*}
we just need to find the minimal value of the 1D function:
\[
f\left(x\right)=x\ln\left(x\right)-x+cx,x>0
\]
\[
\frac{\partial f\left(x\right)}{\partial x}=1\cdot\ln\left(x\right)+x\cdot\frac{1}{x}-1+c=\ln\left(x\right)+1-1+c=\ln\left(x\right)+c
\]
\[
\frac{\partial^{2}f\left(x\right)}{\partial x^{2}}=\frac{1}{x}>0
\]

$f\left(x\right)$ has a P.D hessian over the open set $x>0$, thus
it is convex.

the constraint $x>0$ defines a convex set.

thus this is a convex optimization problem hence any local minimum
is a global minimum.

since $f\left(x\right)$ is continuously differentiable, and all points
in the domain are interior points, the stationarity is necessary for
local optimality, which in this case is a global optimality.

let's find the stationery points:
\[
\frac{\partial f\left(x\right)}{\partial x}=\ln\left(x\right)+c=0
\]
\[
\ln\left(x\right)=-c
\]
\[
x=e^{-c}>0
\]
and the minimum value is:
\[
f\left(e^{-c}\right)=e^{-c}\ln\left(e^{-c}\right)-e^{-c}+ce^{-c}=-e^{-c}
\]
thus:
\[
\min_{x\in X}\sum_{i=1}^{n}\left(x_{i}\ln\left(x_{i}\right)-x_{i}+\lambda^{T}A_{:,i}x_{i}\right)=-\sum_{i=1}^{n}e^{-\lambda^{T}A_{:,i}}
\]
and:
\[
q\left(\lambda\right)=-\sum_{i=1}^{n}e^{-\lambda^{T}A_{:,i}}-\lambda^{T}b
\]

\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
and the dual problem is:
\begin{align*}
\max_{\lambda} & -\sum_{i=1}^{n}e^{-\lambda^{T}A_{:,i}}-\lambda^{T}b\\
s.t & \lambda\geq0
\end{align*}
%
\end{minipage}}

\subsection*{\newpage}

\section*{\uline{Problem 3:}}

Consider the following optimization problem in the variables $\alpha\in\mathbb{R}$
and $q\in\mathbb{R}^{n}$:

\begin{align*}
\min_{\alpha,q} & \alpha\\
s.t & Aq=\alpha f\\
 & \left\Vert q\right\Vert ^{2}\leq\epsilon
\end{align*}
for $A\in\mathbb{R}^{mxn}$ ,$f\in\mathbb{R}^{m}$ and $\epsilon>0$.

\subsection*{a)}

The objective function is a linear function of the variables and hence
convex.

The first $m$ constraints are linear and thus define a convex set.

the 2 norm is a convex function hence the third constraint defines
a convex set as a level set of a convex function.

1) thus this is a convex optimization problem.

notice that $q$ is bounded but also $\alpha$:
\[
\alpha f_{j}=\sum_{i=1}^{n}A_{j,i}q_{i}
\]
\[
\alpha=\frac{\sum_{i=1}^{n}A_{j,i}q_{i}}{f_{j}}
\]
\[
\left|\alpha\right|\leq\frac{\sum_{i=1}^{n}\left|A_{j,i}\right|\sqrt{\epsilon}}{\left|f_{j}\right|}
\]
2) thus the constraints define a closed and bounded set, hence compact.
the objective function is continuously differentiable, hence according
to Weierstrass theorem, it attains a minimal value over the set.

3) Slater condition holds, for example for $\alpha=0$,$q=0_{n}$:
\[
Aq=\alpha f=0_{n}
\]
\[
\left\Vert q\right\Vert ^{2}=0<\epsilon
\]

\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
all three conditions are met, thus strong duality holds.%
\end{minipage}}

\subsection*{b)}

define:
\[
X=\left\{ \alpha\in\mathbb{R},q\in\mathbb{R}^{n}:\left\Vert q\right\Vert ^{2}\leq\epsilon\right\} 
\]

define the Lagrangian:
\[
L\left(\alpha,q,\lambda\right)=\alpha+\mu^{T}\left(Aq-\alpha f\right)
\]

\[
\mu\in\mathbb{R}^{m}
\]

let's find the dual problem:

\begin{align*}
q\left(\mu\right) & =\min_{\left(\alpha,q\right)\in X}L\left(\alpha,q,\mu\right)=\min_{\left(\alpha,q\right)\in X}\alpha+\mu^{T}\left(Aq-\alpha f\right)=\\
 & =\min_{\left(\alpha,q\right)\in X}\alpha-\alpha\mu^{T}f+\mu^{T}Aq=\min_{\alpha\in\mathbb{R}}\left(1-\mu^{T}f\right)\alpha+\min_{\left\Vert q\right\Vert ^{2}\leq\epsilon}\mu^{T}Aq
\end{align*}
\[
\min_{\alpha\in\mathbb{R}}\left(1-\lambda^{T}f\right)\alpha=\begin{cases}
0 & \mu^{T}f=1\\
-\infty & \mu^{T}f\neq1
\end{cases}
\]
we need to solve:
\begin{align*}
\min_{q\in\mathbb{R}^{m}} & \mu^{T}Aq\\
s.t & \left\Vert q\right\Vert ^{2}\leq\epsilon
\end{align*}

notice that using Cauchy Schwartz:

\[
\left|\mu^{T}Aq\right|\leq\left\Vert A^{T}\mu\right\Vert \left\Vert q\right\Vert 
\]
\[
\mu^{T}Aq\geq-\left\Vert A^{T}\mu\right\Vert \left\Vert q\right\Vert \geq-\sqrt{\epsilon}\left\Vert A^{T}\mu\right\Vert 
\]
and if we choose:
\[
q=-\sqrt{\epsilon}\frac{A^{T}\mu}{\left\Vert A^{T}\mu\right\Vert }
\]
then:
\[
\left\Vert q\right\Vert ^{2}=\left\Vert -\sqrt{\epsilon}\frac{A^{T}\mu}{\left\Vert A^{T}\mu\right\Vert }\right\Vert ^{2}=\epsilon
\]
\[
\mu^{T}Aq=\mu^{T}A-\sqrt{\epsilon}\frac{A^{T}\mu}{\left\Vert A^{T}\mu\right\Vert }=-\sqrt{\epsilon}\frac{\left\Vert A^{T}\mu\right\Vert ^{2}}{\left\Vert A^{T}\mu\right\Vert }=-\sqrt{\epsilon}\left\Vert A^{T}\mu\right\Vert 
\]

thus:
\[
\min_{\left\Vert q\right\Vert ^{2}\leq\epsilon}\mu^{T}Aq=-\sqrt{\epsilon}\left\Vert A^{T}\mu\right\Vert 
\]

notice that for $\mu=0_{m}$:
\[
\mu^{T}f\neq1
\]
and then the domain of $q\left(\mu\right)$ is empty, thus it is not
a problem.

\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
hence the dual problem is:
\begin{align*}
\max_{\mu\in\mathbb{R}^{m}} & -\sqrt{\epsilon}\left\Vert A^{T}\mu\right\Vert \\
s.t & \mu^{T}f=1
\end{align*}
%
\end{minipage}}

\subsection*{c)}

instead of solving the dual problem we can find the optimal solution
of the next problem:

\begin{align*}
\min_{\mu\in\mathbb{R}^{m}} & \sqrt{\epsilon}\left\Vert A^{T}\mu\right\Vert ^{2}\\
s.t & \mu^{T}f=1
\end{align*}
the squared norm is a convex function thus $\left\Vert A^{T}\mu\right\Vert ^{2}$
is convex as a linear change in the variables.

the constraint is linear thus defines a convex set.

hence this is a convex optimization problem and Slater holds as the
constraints are linear, thus:
\[
\left\{ K.K.T\right\} =\left\{ Optimal\right\} 
\]

let's find the K.K.T points:
\[
L\left(\mu,\lambda\right)=\sqrt{\epsilon}\left\Vert A^{T}\mu\right\Vert ^{2}+\lambda\left(\mu^{T}f-1\right)
\]
\[
\lambda\in\mathbb{R}
\]
the K.K.T condition are:
\[
\begin{cases}
\frac{\partial L\left(\mu,\lambda\right)}{\partial\mu}=2\sqrt{\epsilon}AA^{T}\mu+\lambda f=0 & \left(1\right)\\
\mu^{T}f=1 & \left(2\right)
\end{cases}
\]
from $\left(1\right)$:
\[
2\sqrt{\epsilon}AA^{T}\mu+\lambda f=0
\]
\[
2\sqrt{\epsilon}AA^{T}\mu=-\lambda f
\]
\[
AA^{T}\mu=-\frac{\lambda f}{2\sqrt{\epsilon}}
\]

since the rows of $A$ are linearly independent:
\[
\mu=-\frac{\lambda}{2\sqrt{\epsilon}}\left(AA^{T}\right)^{-1}f
\]
plug into $\left(2\right)$:
\[
\mu^{T}f=1
\]
\[
\left(-\frac{\lambda}{2\sqrt{\epsilon}}\left(AA^{T}\right)^{-1}f\right)^{T}f=1
\]
\[
-\frac{\lambda}{2\sqrt{\epsilon}}f^{T}\left(AA^{T}\right)^{-1}f=1
\]

if $f^{T}\left(AA^{T}\right)^{-1}f=0$ the equality doesn't hold,
hence there are no K.K.T points and there are no optimal points. 

I'm not sure what's happening in this case.
\[
\lambda=-\frac{2\sqrt{\epsilon}}{f^{T}\left(AA^{T}\right)^{-1}f}
\]

plug back into $\left(1\right)$:
\[
\mu=-\frac{-\frac{2\sqrt{\epsilon}}{f^{T}\left(AA^{T}\right)^{-1}f}}{2\sqrt{\epsilon}}\left(AA^{T}\right)^{-1}f=\frac{\left(AA^{T}\right)^{-1}f}{f^{T}\left(AA^{T}\right)^{-1}f}
\]

we have found a valid K.K.T point, hence it is an optimal point.

the optimal value is:
\[
-\sqrt{\epsilon}\left\Vert \frac{A^{T}\left(AA^{T}\right)^{-1}f}{f^{T}\left(AA^{T}\right)^{-1}f}\right\Vert 
\]

since strong duality holds, and we saw that the $q$ that minimizes
the Lagrangian satisfies:
\[
q=-\sqrt{\epsilon}\frac{A^{T}\mu}{\left\Vert A^{T}\mu\right\Vert }
\]
\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
then the optimal solution of the primal problem is:
\[
q^{*}=-\sqrt{\epsilon}\frac{\frac{A^{T}\left(AA^{T}\right)^{-1}f}{f^{T}\left(AA^{T}\right)^{-1}f}}{\left\Vert \frac{A^{T}\left(AA^{T}\right)^{-1}f}{f^{T}\left(AA^{T}\right)^{-1}f}\right\Vert }
\]
and the optimal value is the same as the dual problem:
\[
\alpha^{*}=-\sqrt{\epsilon}\left\Vert \frac{A^{T}\left(AA^{T}\right)^{-1}f}{f^{T}\left(AA^{T}\right)^{-1}f}\right\Vert 
\]
%
\end{minipage}}

\subsection*{\newpage}

\section*{\uline{Problem 4:}}

Consider the primal optimization problem:
\begin{align*}
\min_{x\in\mathbb{R}^{n}} & \sum_{i=1}^{n}\frac{c_{j}}{x_{i}}\\
s.t & a^{T}x\leq b\\
 & x>0
\end{align*}

where $a\in\mathbb{R}_{++}^{n}$,$c\in\mathbb{R}_{++}^{n}$ and $b\in\mathbb{R}_{++}$

\subsection*{a)}

define:
\[
X=\left\{ x\in\mathbb{R}^{n}:x>0\right\} 
\]

the Lagrangian:
\[
L\left(x,\lambda\right)=\sum_{i=1}^{n}\frac{c_{i}}{x_{i}}+\lambda\left(a^{T}x-b\right)
\]
\[
\lambda\in\mathbb{R}_{+}
\]

let's find the dual problem:
\begin{align*}
q\left(\lambda\right) & =\min_{x\in X}L\left(x,\lambda\right)=\min_{x\in X}\sum_{i=1}^{n}\frac{c_{i}}{x_{i}}+\lambda\left(a^{T}x-b\right)=\\
 & =\min_{x\in X}\sum_{i=1}^{n}\frac{c_{i}}{x_{i}}+\lambda a^{T}x-\lambda b=\min_{x\in X}\sum_{i=1}^{n}\frac{c_{i}}{x_{i}}+\lambda\sum_{i=1}^{n}a_{i}x_{i}-\lambda b=\\
 & =\min_{x\in X}\sum_{i=1}^{n}\left(\frac{c_{i}}{x_{i}}+\lambda a_{i}x_{i}\right)-\lambda b
\end{align*}
this is a separable problem, we just need to find the minimum of the
function:
\[
f\left(x_{i}\right)=\frac{c_{i}}{x_{i}}+\lambda a_{i}x_{i}
\]
where $x_{i}>0,c_{i}>0,\lambda\geq0,a_{i}>0$
\[
f'\left(x_{i}\right)=-\frac{c_{i}}{x_{i}^{2}}+\lambda a_{i}
\]
\[
f''\left(x_{i}\right)=\frac{2c_{i}}{x_{i}^{3}}>0
\]
the function $f$ is twice continuously differentiable and has a P.D
hessian for all $x_{i}$ in the open set $x_{i}>0$ and thus is convex.
the domain is open, thus any point is an interior point if we find
a stationery point it is necessarily the optimal point.

\[
f'\left(x_{i}\right)=-\frac{c_{i}}{x_{i}^{2}}+\lambda a_{i}=0
\]
\[
\frac{c_{i}}{x_{i}^{2}}=\lambda a_{i}
\]

if $\lambda>0$:
\[
x_{i}^{2}=\frac{c_{i}}{\lambda a_{i}}
\]

\[
x_{i}=\sqrt{\frac{c_{i}}{\lambda a_{i}}}>0
\]

and the minimal value is:
\[
f\left(\sqrt{\frac{c_{i}}{\lambda a_{i}}}\right)=\frac{c_{i}}{\sqrt{\frac{c_{i}}{\lambda a_{i}}}}+\lambda a_{i}\sqrt{\frac{c_{i}}{\lambda a_{i}}}=\sqrt{\lambda a_{i}c_{i}}+\sqrt{\lambda a_{i}c_{i}}=2\sqrt{\lambda a_{i}c_{i}}
\]
if $\lambda=0$ then the function becomes:
\[
f\left(x_{i}\right)=\frac{c_{i}}{x_{i}}
\]
which his minimal value is $0$ but it is not attained.

thus:
\[
q\left(\lambda\right)=\begin{cases}
2\sqrt{\lambda}\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}-\lambda b & \lambda>0\\
0 & \lambda=0
\end{cases}=2\sqrt{\lambda}\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}-\lambda b
\]
and the dual problem is:
\begin{align*}
\max_{\lambda\in\mathbb{R}} & \left(2\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}\right)\sqrt{\lambda}-\lambda b\\
s.t & \lambda\geq0
\end{align*}


\subsection*{b)}

we can write the problem as:
\begin{align*}
\min_{\lambda\in\mathbb{R}} & g\left(\lambda\right)=-\left(2\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}\right)\sqrt{\lambda}+\lambda b\\
s.t & \lambda\geq0
\end{align*}
the problem is convex as the dual problem is always convex.

at $\lambda=0$ the function value is $0$.

at any other point the function is convex, continuously differentiable
over a convex set hence if we find a stationary point it is the optimal
point.
\[
g'\left(\lambda\right)=-\frac{\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}}{\sqrt{\lambda}}+b=0
\]
\[
\frac{\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}}{\sqrt{\lambda}}=b
\]
\[
\sqrt{\lambda}=\frac{\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}}{b}
\]
\[
\lambda=\frac{\left(\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}\right)^{2}}{b^{2}}
\]
and the function value is:
\begin{align*}
g\left(\frac{\left(\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}\right)^{2}}{b^{2}}\right) & =-\left(2\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}\right)\frac{\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}}{b}+\frac{\left(\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}\right)^{2}}{b^{2}}b=\\
 & =-\frac{2\left(\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}\right)^{2}}{b}+\frac{\left(\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}\right)^{2}}{b}=-\frac{\left(\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}\right)^{2}}{b}<0
\end{align*}

\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
we found a stationery point, thus it is an optimal point, also it
achives a lower value then at $\lambda=0$ hence this is the global
minimum of the function:
\[
\lambda^{*}=\frac{\left(\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}\right)^{2}}{b^{2}}
\]
\[
q\left(\lambda^{*}\right)=\frac{\left(\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}\right)^{2}}{b}
\]
%
\end{minipage}}

now we need to check if strong duality holds.

for $x>0$ the objective function is a sum of convex functions asnd
hence convex.

the first and second constraints are linear hence define a convex
set.

1) thus this is a convex optimization problem.

2) $\sum_{i=1}^{n}\frac{c_{j}}{x_{i}}>0$, thus $f^{*}>-\infty$

3) Slater condition holds as the constraints are linear.

\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
thus strong duality holds, and we have found that the values that
minimizes the Lagrangian are:
\[
x_{i}^{*}=\frac{1}{\sqrt{\lambda}}\sqrt{\frac{c_{i}}{a_{i}}}=\frac{1}{\frac{\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}}{b}}\sqrt{\frac{c_{i}}{a_{i}}}=\frac{b\sqrt{\frac{c_{i}}{a_{i}}}}{\sum_{j=1}^{n}\sqrt{a_{j}c_{j}}}
\]
the optimal value is the same as the dual problem:
\[
f^{*}=\frac{\left(\sum_{i=1}^{n}\sqrt{a_{i}c_{i}}\right)^{2}}{b}
\]
%
\end{minipage}}

\subsection*{\newpage}

\section*{\uline{Problem 5:}}

Consider the optimization problem:
\begin{align*}
\min_{x,y,z\in\mathbb{R}} & -6x+2y+4z^{2}\\
s.t & 2x+2y+z\leq0\\
 & -2x+4y+z^{2}=0\\
 & y\geq0
\end{align*}


\subsection*{a)}

define:
\[
v=\left(\begin{array}{c}
x\\
y\\
z
\end{array}\right)
\]
then the problem becomes:
\begin{align*}
\min_{x,y,z\in\mathbb{R}}f\left(v\right)= & v^{T}\left(\begin{array}{ccc}
0 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 4
\end{array}\right)v+2\left(\begin{array}{ccc}
-3 & 1 & 0\end{array}\right)v\\
s.t & \left(\begin{array}{ccc}
2 & 2 & 1\end{array}\right)v\leq0\\
 & v^{T}\left(\begin{array}{ccc}
0 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 1
\end{array}\right)v+2\left(\begin{array}{ccc}
-1 & 2 & 0\end{array}\right)v=0\\
 & \left(\begin{array}{ccc}
0 & 1 & 0\end{array}\right)v\geq0
\end{align*}

the objective function is a quadratic function with a P.D matrix thus
convex.

the first and third constraints are level sets of linear function
hence convex.

the second constraint is not convex, to see this we can write it as
two constraints:
\[
v^{T}\left(\begin{array}{ccc}
0 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 1
\end{array}\right)v+2\left(\begin{array}{ccc}
-1 & 2 & 0\end{array}\right)v\leq0
\]
\[
v^{T}\left(\begin{array}{ccc}
0 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 1
\end{array}\right)v+2\left(\begin{array}{ccc}
-1 & 2 & 0\end{array}\right)v\geq0
\]
this is a quadratic function with a P.D matrix, thus the first constraint
defines a convex set but the second constraint defines a concave set.

\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
hence the problem is not a convex optimization problem.%
\end{minipage}}

\subsection*{b)}

define:
\[
V=\left\{ v=\left(\begin{array}{c}
x\\
y\\
z
\end{array}\right)\in\mathbb{R}^{3}:y\geq0\right\} 
\]
and define the Lagrangian:
\[
L\left(x,y,z,\lambda,\mu\right)=-6x+2y+4z^{2}+\lambda\left(2x+2y+z\right)+\mu\left(-2x+4y+z^{2}\right)
\]
\[
\lambda\in\mathbb{R}_{+},\mu\in\mathbb{R}
\]
let's find the dual problem:
\begin{align*}
q\left(\lambda,\mu\right) & =\min_{\left(\begin{array}{ccc}
x & y & z\end{array}\right)\in V}L\left(x,y,z,\lambda,\mu\right)=\\
 & =\min_{\left(\begin{array}{ccc}
x & y & z\end{array}\right)\in V}-6x+2y+4z^{2}+\lambda\left(2x+2y+z\right)+\mu\left(-2x+4y+z^{2}\right)\\
 & =\min_{x\in\mathbb{R}}\left(-6x+2\lambda x-2\mu x\right)+\min_{y\in\mathbb{R}_{+}}\left(2y+2\lambda y+4\mu y\right)+\min_{z\in\mathbb{R}}\left(4z^{2}+\lambda z+\mu z^{2}\right)=\\
 & =\min_{x\in\mathbb{R}}\left(2\left(\lambda-\mu-3\right)x\right)+\min_{y\in\mathbb{R}_{+}}\left(2\left(\lambda+2\mu+1\right)y\right)+\min_{z\in\mathbb{R}}\left(\left(\mu+4\right)z^{2}+\lambda z\right)
\end{align*}
\[
\min_{x\in\mathbb{R}}2\left(\lambda-\mu-3\right)x=\begin{cases}
0 & \lambda-\mu-3=0\\
-\infty & \lambda-\mu-3\neq0
\end{cases}
\]
\[
\min_{y\in\mathbb{R}_{+}}2\left(\lambda+2\mu+1\right)y\begin{cases}
0 & \lambda+2\mu+1\geq0\\
-\infty & \lambda+2\mu+1<0
\end{cases}
\]
we need to solve:
\[
\min_{z\in\mathbb{R}}\left(\mu+4\right)z^{2}+\lambda z
\]
for $\mu>-4$ this is a minimum parabola, the minimum is obtained
at:
\[
z=-\frac{\lambda}{2\left(\mu+4\right)}
\]
and the value is:
\begin{align*}
 & \left(\mu+4\right)\left(\frac{\lambda}{2\left(\mu+4\right)}\right)^{2}-\lambda\frac{\lambda}{2\left(\mu+4\right)}=\frac{\left(\mu+4\right)\lambda^{2}}{4\left(\mu+4\right)^{2}}-\frac{\lambda^{2}}{2\left(\mu+4\right)}=\\
 & =\frac{\lambda^{2}}{4\left(\mu+4\right)}-\frac{2\lambda^{2}}{4\left(\mu+4\right)}=-\frac{\lambda^{2}}{4\left(\mu+4\right)}
\end{align*}

for $\mu<-4$ this is a maximum parabola and thus unbounded from below.

if $\mu=-4$ this is a linear function of $z$ that attains a minimum
value only if $\lambda=0$.

to sum:
\[
\min_{z\in\mathbb{R}}\left(\mu+4\right)z^{2}+\lambda z=\begin{cases}
-\frac{\lambda^{2}}{4\left(\mu+4\right)} & \mu>-4\\
0 & \mu=-4,\lambda=0\\
-\infty & \mu<-4
\end{cases}
\]
notice that if $\mu=-4$ and $\lambda=0$ then:
\[
\lambda-\mu-3=0+4-3=1\neq0
\]
hence $x$ won't have a minimal value, thus it is not possible.

in addition if $\lambda-\mu-3=0$

then:
\[
\lambda=\mu+3
\]

and from:
\[
\lambda+2\mu+1\geq0
\]
we get:
\[
\lambda+2\mu+1=\mu+3+2\mu+1=3\mu+4\geq0
\]
\[
3\mu\geq-4
\]
\[
\mu\geq-\frac{4}{3}>-4
\]
hence only the first domain if $z$ is relevant.

\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
to sum the dual problem is:
\begin{align*}
\max_{\lambda,\mu\in\mathbb{R}} & -\frac{\lambda^{2}}{4\left(\mu+4\right)}\\
s.t & \lambda-\mu-3=0\\
 & \lambda+2\mu+1\geq0\\
 & \lambda\geq0
\end{align*}
%
\end{minipage}}

\subsection*{c)}

we can write the problem as a 1D problem in $\lambda$:
\[
\mu=\lambda-3
\]
\[
\lambda+2\mu+1=\lambda+2\lambda-6+1=3\lambda-5\geq0\rightarrow3\lambda\geq5\rightarrow\lambda\geq\frac{5}{3}
\]
we can write the problem as:
\begin{align*}
\min_{\lambda} & g\left(\lambda\right)=\frac{\lambda^{2}}{4\left(\lambda+1\right)}\\
s.t & \lambda\geq\frac{5}{3}
\end{align*}
this is a quadratic over linear function and hence convex.

the constraint is convex as it is a level set of a linear function.

hence this is a convex optimization problem, if we find a stationary
point it is the optimal point.
\begin{align*}
g'\left(\lambda\right) & =\frac{8\lambda\left(\lambda+1\right)-4\lambda^{2}}{16\left(\lambda+1\right)^{2}}=\frac{8\lambda^{2}+8-4\lambda^{2}}{16\left(\lambda+1\right)^{2}}=\frac{4\lambda^{2}+8}{16\left(\lambda+1\right)^{2}}=\\
 & =\frac{\lambda^{2}+2}{4\left(\lambda+1\right)^{2}}=0
\end{align*}
\[
\lambda^{2}+2=0
\]
hence there are no stationery points.

however, the function is coercive on the domain $\lambda\geq\frac{5}{3}$
thus it must attain a minimum on the closed set.

\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
the minimum has to be attained on the boundary:
\[
\lambda^{*}=\frac{5}{3}
\]
\[
\mu^{*}=\frac{5}{3}-3=-\frac{4}{3}
\]
the optimal value of the dual problem is:
\[
-\frac{\lambda^{2}}{4\left(\mu+4\right)}=-\frac{\left(\frac{5}{3}\right)^{2}}{4\left(-\frac{4}{3}+4\right)}=-\frac{25}{96}=-0.26
\]
%
\end{minipage}}

\subsection*{d)}

\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
since the original problem is not a convex optimization problem, strong
duality doesn't necessary hold, so the optimal solution of the dual
problem is only a lower bound on the optimal solution of the primal
problem.%
\end{minipage}}

\subsection*{e)}

using:
\[
-2x+4y+z^{2}=0
\]
we get:
\[
2x=4y+z^{2}
\]
\[
x=2y+\frac{1}{2}z^{2}
\]
we can write a new problem:
\begin{align*}
\min_{y,z\in\mathbb{R}} & -12y-3z^{2}+2y+4z^{2}=z^{2}-10y\\
s.t & 6y+z+z^{2}\leq0\\
 & y\geq0
\end{align*}

the objective function is a quadratic function with a P.D matrix hence
convex.

the first constraint is a level set of a quadratic function with a
P.D matrix hence defines a convex set.

that second constraint is a level set of a linear function hence convex.

thus now the problem is a convex optimization problem.

now define:
\[
V=\left\{ v=\left(\begin{array}{c}
y\\
z
\end{array}\right)\in\mathbb{R}^{2}:y\geq0\right\} 
\]
write the new Lagrangian:
\[
L\left(y,z,\lambda\right)=z^{2}-10y+\lambda\left(6y+z+z^{2}\right)
\]
\[
\lambda\in\mathbb{R}_{+}
\]
let's find the dual problem:
\begin{align*}
q\left(\lambda\right) & =\min_{\left(\begin{array}{cc}
y & z\end{array}\right)\in V}L\left(y,z,\lambda\right)=\\
 & =\min_{\left(\begin{array}{cc}
y & z\end{array}\right)\in V}z^{2}-10y+\lambda\left(6y+z+z^{2}\right)=\\
 & =\min_{y\in\mathbb{R}_{+}}6\lambda y-10y+\min_{z\in\mathbb{R}}z^{2}+\lambda z+\lambda z^{2}=\\
 & =\min_{y\in\mathbb{R}_{+}}2\left(3\lambda-5\right)y+\min_{z\in\mathbb{R}}\left(\lambda+1\right)z^{2}+\lambda z
\end{align*}
\[
\min_{y\in\mathbb{R}_{+}}2\left(3\lambda-5\right)y=\begin{cases}
0 & \lambda\geq\frac{5}{3}\\
-\infty & \lambda<\frac{5}{3}
\end{cases}
\]
we need to solve:
\[
\min_{z\in\mathbb{R}}\left(\lambda+1\right)z^{2}+\lambda z
\]
$\lambda+1>0$ hence this is a minimum parabola, and the optimal value
is attained at:
\[
z=-\frac{\lambda}{2\left(\lambda+1\right)}
\]
and the optimal value is:
\begin{align*}
 & \left(\lambda+1\right)\left(-\frac{\lambda}{2\left(\lambda+1\right)}\right)^{2}-\lambda\frac{\lambda}{2\left(\lambda+1\right)}=\\
 & \frac{\lambda^{2}\left(\lambda+1\right)}{4\left(\lambda+1\right)^{2}}-\frac{\lambda^{2}}{2\left(\lambda+1\right)}=\frac{\lambda^{2}}{4\left(\lambda+1\right)}-\frac{2\lambda^{2}}{4\left(\lambda+1\right)}=-\frac{\lambda^{2}}{4\left(\lambda+1\right)}
\end{align*}
\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
thus the dual problem is:
\begin{align*}
\max_{\lambda\in\mathbb{R}} & -\frac{\lambda^{2}}{4\left(\lambda+1\right)}\\
s.t & \lambda\geq\frac{5}{3}
\end{align*}
%
\end{minipage}}

\subsection*{f)}

1) the new problem is a convex optimization problem as we saw in the
last section.

2) the objective function is a quadratic function with a P.D matrix
hence bounded from below, meaning $f^{*}>-\infty$

3) Slater condition holds, for example for $y=0$ and $z=-\frac{1}{2}$
\[
6y+z+z^{2}=-\frac{1}{2}+\frac{1}{4}=-\frac{1}{4}<0
\]
\[
y\geq0\text{(slater only aplies on the non liniear constraints)}
\]

all three condition are met, thus strong duality holds, thus we can
derive the solution of the primal problem from the dual problem now.

the dual problem is:
\begin{align*}
\max_{\lambda\in\mathbb{R}} & -\frac{\lambda^{2}}{4\left(\lambda+1\right)}\\
s.t & \lambda\geq\frac{5}{3}
\end{align*}
we actually solved this problem is section $c$ and saw that

the minimum has to be attained on the boundary:
\[
\lambda^{*}=\frac{5}{3}
\]
the optimal value of the dual problem is:
\[
-\frac{\lambda^{2}}{4\left(\mu+4\right)}=-\frac{\left(\frac{5}{3}\right)^{2}}{4\left(-\frac{4}{3}+4\right)}=-\frac{25}{96}=-0.26
\]
\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
now from strong duality we can derive that the optimal solution of
the primal problem is attained at:
\[
z^{*}=-\frac{\lambda}{2\left(\lambda+1\right)}=-\frac{\frac{5}{3}}{2\left(\frac{5}{3}+1\right)}=-\frac{5}{16}
\]
and from complementary slackness:
\[
\lambda^{*}\left(6y+z+z^{2}\right)=0
\]
\[
6y+z+z^{2}=0
\]
\[
y^{*}=-\frac{z^{*2}+z^{*}}{6}=\frac{55}{1536}
\]

\[
x^{*}=2y^{*}+\frac{1}{2}z^{*2}=\frac{185}{1536}
\]
the optimal value is the same as the dual problem:
\[
-\frac{25}{96}
\]
%
\end{minipage}}
\end{document}
